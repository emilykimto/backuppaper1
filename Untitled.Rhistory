HISTORY:

library(opendatatoronto)
library(tidyverse)
library(knitr)
library(dplyr)
library(ggplot2)
library(lubridate)
library(janitor)
library(tibble)
#### Acquire the dataset from Toronto Open Data ####
language_services <-
# https://open.toronto.ca/dataset/paramedic-services-911-language-interpretation/
list_package_resources("42315239-36a8-4b7f-b2ab-6ab60fb0b935") |>
filter(name == "Language Services Data (2014 - 2021)") |>
get_resource()
#### Saving the dataset ####
write_csv(
x = language_services,
file = "language_services.csv"
)
#### Separate the Time Stamp column into two columns for Date and Time ####
language_services_tbl <-
as_tibble(language_services)
language_services_separated <-
language_services_tbl |>
separate("Time Stamp", into = c("Date", "Time"), sep = " ")
head(language_services_separated)
#### Saving cleaned dataset ####
write_csv(
x = language_services_separated,
file = "cleaned_language_services.csv"
)
#### Calculate the sum of call duration for each language ####
language_services_summary <-
language_services |>
group_by(Language) |>
summarize(total_duration = sum(Duration))
head(language_services_summary)
#### Workspace Set-up ####
# install.packages("opendatatoronto")
# install.packages("tidyverse")
# install.packages("ggplot2")
# install.packages("knitr")
# install.packages("lubridate")
# install.packages("dplyr")
library(opendatatoronto)
library(tidyverse)
library(knitr)
library(dplyr)
library(ggplot2)
library(lubridate)
library(janitor)
library(tibble)
#### Acquire the dataset from Toronto Open Data ####
language_services <-
# https://open.toronto.ca/dataset/paramedic-services-911-language-interpretation/
list_package_resources("42315239-36a8-4b7f-b2ab-6ab60fb0b935") |>
filter(name == "Language Services Data (2014 - 2021)") |>
get_resource()
#### Saving the dataset ####
write_csv(
x = language_services,
file = "language_services.csv"
)
#### Separate the Time Stamp column into two columns for Date and Time ####
language_services_tbl <-
as_tibble(language_services)
language_services_separated <-
language_services_tbl |>
separate("Time Stamp", into = c("Date", "Time"), sep = " ")
head(language_services_separated)
#### Saving cleaned dataset ####
write_csv(
x = language_services_separated,
file = "cleaned_language_services.csv"
)
#### Calculate the sum of call duration for each language ####
language_services_summary <-
language_services |>
group_by(Language) |>
summarize(total_duration = sum(Duration))
head(language_services_summary)
#### Find the top 10 languages that require language services ####
top_10_languages <-
language_services_summary |>
arrange(desc(total_duration)) |>
top_n(10)
head(top_10_languages)
language_services_summary <-
language_services |>
group_by(Language) |>
summarize(total_duration = sum(Duration))
head(language_services_summary)
#### Rename "total_duration" column to "Total Duration" ####
language_services_summary <-
language_services_summary |>
rename("Total Duration" = total_duration)
#### Find the top 10 languages that require language services ####
top_10_languages <-
language_services_summary |>
arrange(desc("Total Duration")) |>
top_n(10)
head(top_10_languages)
library(opendatatoronto)
library(janitor)
library(lubridate)
library(tidyverse)
#### Simulate ####
set.seed(853)
simulated_languageduration_data <-
tibble(
date = rep(x = as.Date("2014-01-01") + c(0:364*7), times = 3),
language = c(
rep(x = "Language 1", times = 365*8),
rep(x = "Language 2", times = 365*8),
rep(x = "Language 3", times = 365*8)
),
call_duration =
rpois(
n = 365*8*3,
lambda = 15
)
)
set.seed(853)
simulated_languageduration_data <-
tibble(
date = rep(x = as.Date("2014-01-01") + c(0:364*7), times = 365*8*3),
language = c(
rep(x = "Language 1", times = 365*8),
rep(x = "Language 2", times = 365*8),
rep(x = "Language 3", times = 365*8)
),
call_duration =
rpois(
n = 365*8*3,
lambda = 15
)
)
set.seed(853)
simulated_languageduration_data <-
tibble(
date = rep(x = as.Date("2014-01-01") + c(0:364), times = 3),
language = rep(c("Language 1", "Language 2", "Language 3"), each = 365),
call_duration =
rpois(
n = 365*3,
lambda = 15
)
) |>
rep(each = 8)
head(simulated_languageduration_data)
set.seed(853)
simulated_languageduration_data <-
tibble(
date = rep(x = as.Date("2014-01-01") + c(0:364), times = 3),
language = rep(c("Language 1", "Language 2", "Language 3"), each = 365),
call_duration =
rpois(
n = 365*3,
lambda = 15
)
) |>
rep(each = 8)
head(simulated_languageduration_data)
set.seed(853)
simulated_languageduration_data <-
tibble(
date = rep(x = as.Date("2014-01-01") + c(0:364), times = 3),
language = c(
rep(x = "Language 1", times = 365),
rep(x = "Language 2", times = 365),
rep(x = "Language 3", times = 365)
),
call_duration =
rpois(
n = 365*3,
lambda = 15
)
)
head(simulated_languageduration_data)
head(simulated_languageduration_data)
git-pull(1)
rm -fr ".git/rebase-merge"
git config pull.rebase false  # merge
git rebase (--continue | --abort | --skip)
-fr ".git/rebase-merge"
rm -fr ".git/rebase-merge"
git rebase (--continue )
git.rebase (--continue )
git pull --ff-only
source("~/languageservices/scripts/scripts:00-download_data.R")
savehistory("~/languageservices/Untitled.Rhistory")



DOWNLOAD_DATA.R code (Jan 29):

#### Preamble ####
# Purpose: Clean Language Services data downloaded from Toronto Open Data
# Author: Emily Kim
# Date: 28 January 2023
# Contact: emilyuna.kim@mail.utoronto.ca
# License: MIT
# Pre-req: None

#### Workspace Set-up ####
# install.packages("opendatatoronto")
# install.packages("tidyverse")
# install.packages("dplyr")
# install.packages("ggplot2")
# install.packages("knitr")
# install.packages("lubridate")

library(opendatatoronto)
library(tidyverse)
library(knitr)
library(dplyr)
library(ggplot2)
library(lubridate)
library(janitor)
library(tibble)

#### Acquire the dataset from Toronto Open Data ####
language_services <-
  # https://open.toronto.ca/dataset/paramedic-services-911-language-interpretation/
  list_package_resources("42315239-36a8-4b7f-b2ab-6ab60fb0b935") |>
  filter(name == "Language Services Data (2014 - 2021)") |>
  get_resource()

#### Saving the dataset ####
write_csv(
  x = language_services,
  file = "language_services.csv"
)

#### Separate the Time Stamp column into two columns for Date and Time ####
language_services_tbl <-
  as_tibble(language_services)

language_services_separated <-
  language_services_tbl |>
  separate("Time Stamp", into = c("Date", "Time"), sep = " ")

head(language_services_separated)

#### Saving cleaned dataset ####
write_csv(
  x = language_services_separated,
  file = "cleaned_language_services.csv"
)

#### Calculate the sum of call duration for each language ####
language_services_summary <-
  language_services |>
  group_by(Language) |>
  summarize(total_duration = sum(Duration))

head(language_services_summary)

#### Rename "total_duration" column to "Total Duration" ####
language_services_summary <-
  language_services_summary |>
  rename("Total Duration" = total_duration)

#### Find the top 10 languages that require language services ####
top_10_languages <-
  language_services_summary |>
  arrange(desc("Total Duration")) |>
  top_n(10)

head(top_10_languages)




SIMULATION.R code (JAN 29):

#### Preamble ####
# Purpose: Get data on Toronto Language Services data during the 2021 year and make a table
# Author: Emily Kim
# Email: emilyuna.kim@mail.utoronto.ca
# Date: 28 January 2023
# Pre-req: None

#### Workspace set-up ####
install.packages("opendatatoronto")
install.packages("tidyverse")

library(opendatatoronto)
library(janitor)
library(lubridate)
library(tidyverse)

#### Simulate ####
set.seed(853)
simulated_languageduration_data <-
  tibble(
    date = rep(x = as.Date("2014-01-01") + c(0:364), times = 3),
    language = c(
      rep(x = "Language 1", times = 365),
      rep(x = "Language 2", times = 365),
      rep(x = "Language 3", times = 365)
    ),
    call_duration =
      rpois(
        n = 365*3,
        lambda = 15
      )
  )
head(simulated_languageduration_data)
